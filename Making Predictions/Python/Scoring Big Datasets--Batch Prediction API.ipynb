{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Big Datasets - Batch Prediction API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope\n",
    "\n",
    "The scope of this notebook is to provide instructions on how to use DataRobot's batch prediction script to get predictions out of a DataRobot deployed model\n",
    "\n",
    "### Background\n",
    "\n",
    "The Batch Prediction API provides flexible options for intake and output when scoring large datasets using the prediction servers you have already deployed. The API is exposed through the DataRobot Public API and can be consumed using a REST-enabled client or the DataRobot Python Public API bindings.\n",
    "\n",
    "The main features of the API include:\n",
    "\n",
    "- Flexible options for intake and output.\n",
    "- Support for streaming local files and the ability to start scoring while still uploading—while simultaneously downloading the results.\n",
    "- Ability to score large datasets from, and to, Amazon S3 buckets.\n",
    "- Connection to external data sources using JDBC with bidirectional streaming of scoring data and results.\n",
    "- A mix of intake and output options; for example, scoring from a local file to an S3 target.\n",
    "- Protection against prediction server overload with a concurrency control level option.\n",
    "- Inclusion of Prediction Explanations (with an option to add thresholds).\n",
    "- Support for passthrough columns to correlate scored data with source data.\n",
    "- Addition of prediction warnings in the output.\n",
    "\n",
    "(If you are a DataRobot customer, you can find more information in the in-app documentation for \"Batch Prediction API.\")\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Python version 3.7.3\n",
    "-  DataRobot API version 2.19.0. \n",
    "\n",
    "Small adjustments might be needed depending on the Python version and DataRobot API version you are using.\n",
    "\n",
    "Full documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com/en/\n",
    "\n",
    "It is assumed you already have a DataRobot <code>Deployment</code> object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring a local CSV file\n",
    "\n",
    "For the below script to work, you will have to provide DataRobot with the <code>api_key</code> (of your account), the <code>deployment_id</code>, and the <code>api_endpoint</code> (which should be https://app.datarobot.com/api/v2/batchPredictions/ for NA Managed AI Cloud users and https://app.eu.datarobot.com/api/v2/batchPredictions/ for EU Managed AI Cloud users).\n",
    "\n",
    "Lastly, you need to provide the CSV file that will be used as input.\n",
    "\n",
    "By default, DataRobot will expect a CSV file with this format:\n",
    "\n",
    "- delimiter Default: , (comma).\n",
    "- quotechar: \"\n",
    "- encoding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "api_key = '...'\n",
    "api_endpoint = 'https://app.datarobot.com/api/v2/batchPredictions/'\n",
    "\n",
    "deployment_id = '...'\n",
    "\n",
    "csv_input_file = './to_predict.csv'\n",
    "csv_output_file = './predicted.csv'\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers = {\n",
    "    'Authorization': 'Bearer {}'.format(api_key),\n",
    "}\n",
    "\n",
    "job_details = {\n",
    "    'deploymentId': deployment_id,\n",
    "    'intakeSettings': {'type': 'local_file'},\n",
    "    'outputSettings': {'type': 'local_file'},\n",
    "}\n",
    "\n",
    "# Initialize the job\n",
    "resp = session.post(api_endpoint, json=job_details)\n",
    "resp.raise_for_status()\n",
    "\n",
    "job = resp.json()\n",
    "\n",
    "print(\"created job: {}\".format(job['links']['self']))\n",
    "\n",
    "# Feed the CSV data\n",
    "with open(csv_input_file, 'rb') as f:\n",
    "    resp = session.put(job['links']['csvUpload'], data=f, headers={\n",
    "        'Content-Type': 'text/csv'\n",
    "    })\n",
    "    resp.raise_for_status()\n",
    "\n",
    "# Wait for processing to start\n",
    "while not (job['status'] == 'ABORTED' or job['links'].get('download')):\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    resp = session.get(job['links']['self'])\n",
    "    resp.raise_for_status()\n",
    "    job = resp.json()\n",
    "\n",
    "if job['status'] == 'ABORTED':\n",
    "\n",
    "    print('failed to complete batch predictions: {}', job['statusDetails'])\n",
    "\n",
    "else:\n",
    "\n",
    "    # Download the results\n",
    "    resp = session.get(job['links']['download'], stream=True)\n",
    "    resp.raise_for_status()\n",
    "    with open(csv_output_file, 'wb') as f:\n",
    "         for chunk in resp.iter_content(chunk_size=8192):\n",
    "             if chunk:\n",
    "                 f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requesting Prediction Explanations\n",
    "In order to get Prediction Explanations alongside predictions, you need to change the job configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details = {\n",
    "    'deploymentId': deployment_id,\n",
    "    'intakeSettings': {'type': 'local_file'},\n",
    "    'outputSettings': {'type': 'local_file'},\n",
    "    'maxExplanations': 10,\n",
    "    'thresholdHigh': 0.5,\n",
    "    'thresholdLow': 0.15,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom CSV Format\n",
    "If your CSV file does not match the custom CSV format, you can modify the expected CSV format by setting <code>csvSettings</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details = {\n",
    "    'deploymentId': deployment_id,\n",
    "    'intakeSettings': {'type': 'local_file'},\n",
    "    'outputSettings': {'type': 'local_file'},\n",
    "    'csvSettings': {\n",
    "        'delimiter': ';',\n",
    "        'quotechar': '\\'',\n",
    "        'encoding': 'ms_kanji',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End-to-end scoring of CSV files on S3 using Python requests\n",
    "\n",
    "To score data that sits in an S3 Bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "api_key = '...'\n",
    "api_endpoint = 'https://app.datarobot.com/api/v2/batchPredictions/'\n",
    "\n",
    "deployment_id = '...'\n",
    "credential_id = '...'\n",
    "\n",
    "s3_csv_input_file = 's3://my-bucket/data/to_predict.csv'\n",
    "s3_csv_output_file = 's3://my-bucket/data/predicted.csv'\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers = {\n",
    "    'Authorization': 'Bearer {}'.format(api_key),\n",
    "}\n",
    "\n",
    "job_details = {\n",
    "    'deploymentId': deployment_id,\n",
    "    'intakeSettings': {\n",
    "        'type': 's3',\n",
    "        'url': s3_csv_input_file,\n",
    "        'credentialId': credential_id,\n",
    "    },\n",
    "    'outputSettings': {\n",
    "        'type': 's3',\n",
    "        'url': s3_csv_output_file,\n",
    "        'credentialId': credential_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send the job\n",
    "resp = session.post(api_endpoint, json=job_details)\n",
    "resp.raise_for_status()\n",
    "\n",
    "job = resp.json()\n",
    "\n",
    "print('queued batch job: {}'.format(job['links']['self']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can poll the status (self) endpoint to check for progress or wait until the job is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while job['status'] not in {'COMPLETED', 'ABORTED'}:\n",
    "\n",
    "    time.sleep(5)\n",
    "        \n",
    "    resp = session.get(job['links']['self'])\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    job = resp.json()\n",
    "\n",
    "print('job finished with status: {}'.format(job['status']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can include Prediction Explanations by adding the desired Prediction Explanation parameters to the job configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details = {\n",
    "    'deploymentId': deployment_id,\n",
    "    'intakeSettings': {\n",
    "        'type': 's3',\n",
    "        'url': s3_csv_input_file,\n",
    "        'credentialId': credential_id,\n",
    "    },\n",
    "    'outputSettings': {\n",
    "        'type': 's3',\n",
    "        'url': s3_csv_output_file,\n",
    "        'credentialId': credential_id,\n",
    "    }\n",
    "    'maxExplanations': 10,\n",
    "    'thresholdHigh': 0.5,\n",
    "    'thresholdLow': 0.15,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End-to-end scoring from a JDBC PostgreSQL database using Python requests \n",
    "The following reads a scoring dataset from the table public.scoring_data and saves the scored data back to public.scored_data (assuming that table already exists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "\n",
    "api_key = '...'\n",
    "api_endpoint = 'https://app.datarobot.com/api/v2/batchPredictions/'\n",
    "\n",
    "deployment_id = '...'\n",
    "credential_id = '...'\n",
    "data_store_id = '...'\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers = {\n",
    "    'Authorization': 'Bearer {}'.format(api_key),\n",
    "}\n",
    "\n",
    "job_details = {\n",
    "    'deploymentId': deployment_id,\n",
    "    'intakeSettings': {\n",
    "        'type': 'jdbc',\n",
    "        'dataStoreId': data_store_id,\n",
    "        'credentialId': credential_id,\n",
    "        'table': 'scoring_data',\n",
    "        'schema': 'public',\n",
    "    },\n",
    "    'outputSettings': {\n",
    "        'type': 'jdbc',\n",
    "        'dataStoreId': data_store_id,\n",
    "        'credentialId': credential_id,\n",
    "        'table': 'scored_data',\n",
    "        'schema': 'public',\n",
    "        'statementType': 'insert'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send the job\n",
    "resp = session.post(api_endpoint, json=job_details)\n",
    "\n",
    "if resp.status_code > 299:\n",
    "    print(resp.json())\n",
    "    sys.exit(-1)\n",
    "\n",
    "job = resp.json()\n",
    "\n",
    "print('queued batch job: {}'.format(job['links']['self']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
